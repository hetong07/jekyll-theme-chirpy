---
title: Paxos RSM
author: hetong07
date: 2020-05-27 15:09:00 -0700
categories: [Blogging, Technical]
tags: [paxos, distributed system, RSM, raft]
---

## Replicated State Machine (RSM)
[1] talks about the theory of RSM.
The core idea if the RSM is straightforward: replicate the state machine (SM) on different processes/servers, and as lone as the input to those SMs are the same, all the RMs should have the same status. The key to this RSM architecture is how to keep the input sequence to all SMs are the same. [1] decompose the requirement of the RSM to:
- **Agreement**: all non-faulty SMs receive every request,
- **Order**: All non-faulty SMs process the request it receives in the same order.

It further discusses those requirements and provides three ways to achieve them.
- Using a logical clock
- Using a synchronized real-time clock
- Using replica-generated identifiers

In the synchronized real-time clock, we have to consider the time uncertainty, which reminds me of the *true time* notion used in Spanner.

Another interesting thing is article [1] divide SM failure into roughly two kinds:
- **Byzantine Failure**: faulty SM can continue to operate,
- **Fail-stop Failure**: faulty SM can detect its error and stop operating.
To achieve the same level of fault-tolerant, the second one needs fewer SMs, but I guess it is hard to implement that SM in a distributed system world and some watchdog mechanisms may be useful if the system is simple.

## Paxos RSM
In paper [2], it talks about how to implement an efficient RSM based on Paxos, and its main competitor is the chubby lock server from Google. The paper's main idea is to use the Paxos to generate the agreement of the input sequence.

The authors explain why the GFS has a relaxed semantics of the RSM since it only supports the append-only write operation rather than the overwrite. As for the Chubby, although it provides the full semantics, it uses the synchronized clock and has a restriction of all status should be fit in memory. Moreover, its performance is not good.

The Gaios proposed by [2] aims to build a system works within local area network, and the LAN bandwidth is usually higher than the out-bound network, that is why it lets the leader forward request to replications,
The separation of data and checksum in LSS may help to detect some data corruption issues within one disk, for example, if there is misdirect read, at lease the mismatch of checksum can tell such error. But I doubt this may incur some overhead in read?

By removing the SQL in the SMART, authors decrease the write for one proposal to two: one for the log, one for the actual operation. They add an extra pointer to the transaction log to eliminate the LSS transaction log write. The idea is simple: since the proposal log has already had all the operations, it is a waste to log those operations in the LSS transaction log again.

The authors spend much of their effort on optimizing disk operations, and their idea is pretty much straightforward:

- For read/write ops, do it in memory and use prefetching to speed it up. It also releases the in-memory write block immediately after it is written. They do it because the ops have already logged, even if it is lost, it can be easily recovered.

-For read-only opes, the leader spreads out to none-checkpointing replications to improve the read bandwidth. By exploiting a novel protocol, which is much similar to the one discussed in [1], it remedies the leadership changing during read ops. One thing to note is spreading read may not gain performance improvement because 1) for random read, there may not be enough read to schedule/optimize on a single server; 2) for the sequential read, it incurs **seek overhead**, as shown in Fig. 4.

How to understand the seek overhead? I guess, for example, there are ten sequential reads, and all the reads are placed consecutively on disk. Read them on one server will save the drive seek time between all the reads, but if reads are spread out, the seek time will emerge. I suspect this is the reason the performance in Fig.4 decreases along with the number of servers. It is a kind of surprise that parallelizing the sequential read will only gain marginal benefit.

I did not anticipate that the non-determinism issue will arise in a deterministic system before reading [2]. One interesting example is the memory allocation error. The way the author solve it is vague, do they mean those error will be solved locally?

The last piece of the paper worth noting is the comparison with the primary-backup system (PB). The PB system requires that the leader should always be the one in the quorum, or to say, the one with all correct history. While, on the contrary, in the Paxos RSM, the leader may not be the one with the correct history. The Paxos mechanism does not have any requirements for the requester.

As for the performance, the system proposed in [1] seems not that good compared to PBN, which is not a surprise. In the evaluation, the paper reports the SQL server's write optimization significantly reduces the time spend on write. It also states that on the amplified server-side write load hides the effect of random read benefit.

## Other things to note
In the evaluation, the authors present a way to calculate the theoretical IO ops for disk, which is the first time to see it, very interesting.

The PB system discussed in [2] reminds me of the Raft. The basic Raft also has a master, and the master node will process all requests. Therefore, the extra bandwidth introduces by replication is wasted. This issue can be solved by spreading the read to replication.  Moreover, the master in Raft also should be from the quorum with the correct history (its mechanism guarantees this). Does it mean the Raft is a kind of PB system? 

On the other side, the basic Raft does not suffer the extra write overhead in the log as the SMART does? Imagine that in the key-value server lab, the Raft log is the only log, and the key-value server is the SM executes the operations produced by Raft. 


## Reference 
1. Schneider, Fred B. "Replication management using the state-machine approach, Distributed systems." (1993).
2. Bolosky, William J., et al. "Paxos replicated state machines as the basis of a high-performance data store." Proc. NSDIâ€™11, USENIX Conference on Networked Systems Design and Implementation. 2011.