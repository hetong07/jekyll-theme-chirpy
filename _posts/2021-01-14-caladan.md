
# Caladan interference mitigation

People are always focus how to schedule tasks within a cluster or across cluster, but less focus on scheduling tasks within a machine, because we usually think the Linux scheduler is highly optimized and too complicated to change.  This paper [Caladan: Mitigating Interference at Microsecond Timescales](https://www.usenix.org/conference/osdi20/presentation/fried), however, focus on how to improve scheduling efficiency on single machine. This work may also be useful for serverless, because according to recent reports most serverless workload has a runtime ~100 ms, and how to fast react to the resource spike would be a very interesting problem.

## Background
The authors argues that state-of-the-art system relies on application-level tail latency measurement is too noisy and/or too slow, and it cannot identify the cause of the interference, not to mention reacting quickly. Besides, they disable hyperthread which causes the machine underutilized.

On the other hand, the hardware support is also not sufficient either. The CAT techniques could partition last level cache(LLC), but it is too slow to take effect. And it also causes performance problem (*Question: why performance suffers ?*); The Memory Bandwidth Allocation (MBA) can limit bandwidth for each cores, but it would cause the core idle (*Question: Idle is not bad, since idle cores could be rescheduled and increase utilization?*). Other techniques, such as, CMT and MBM, is too slow.

## Solution
To identify presence and cause of interference, the authors carefully select a group of signals: 
|signal                 |event                 |Solution                                            |
|-----------------------|----------------------|----------------------------------------------------|
|request processing time|hyperthread contention|disable hyerthread in sibling cores                |
|total memory bandwidth |memory contention     |reduce cores of the cause program                   |
|queue delay            |LLC contention        |Nothing to do, but compensate victim with more cores*|

* *The compensation solution is heavily depends on how the program would benefit from getting more cores, as the paper discussed later that it need programs to be optimized for multi-thread.* 

There is a core reserved primarily for scheduling and (optional) some cores not guaranteed to any tasks. Each running program need to link a runtime library and each core need an extra kernel module to measure& report events.

### Scheduler
Three components work in parallel (omit algorithm here)
1)  core allocator
2)  hyperthread controller
3) memory bandwidth controller

One questions is why LLC is not suitable for measure non-temporal memory access? why non-temporal memory access do not allocate cache?

### Kernel module
Each kernel module is like a agent or RPC server for the scheduling core, it performs commands written in per-core shared memory (64B). The scheduler write to per-core region and kick off a IPI to ask cores to do some job.

To achieve the performance improvement, it uses 1) multicast to many cores at once to reduce IPI overhead, 2) make the IPI response async, so the schedule could proceed. (*Question: will IPI response overwhelm the scheduler core?*) and 3) offloading expensive task to target cores (*Question: What are those situations? e.g., to stop a task, let the target core signal rather than schedule do it?*)

Therefore, the 4 problems identified for Linux are addressed by 1) offload expensive operation to target core, 2) Async to avoid blocking, 3) multicast to amortize IPI overhead and 4) scheduler gain a global view from agents on other cores.

### Others
The package queuing delay is measure directly from NIC's RX queue in shared memory.
User-space package processing.

## Limitation
- Not fully compatible with Linux, since it is built on top of Shenango.
- No NUMA support
- Hyperthread security is not guaranteed.
