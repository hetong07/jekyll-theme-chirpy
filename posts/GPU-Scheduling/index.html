<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="theme" content="Chirpy v2.7.2"><meta name="generator" content="Jekyll v4.2.0" /><meta property="og:title" content="GPU Scheduling for ML workload" /><meta name="author" content="hetong07" /><meta property="og:locale" content="en_US" /><meta name="description" content="GPU Scheduling for ML workload" /><meta property="og:description" content="GPU Scheduling for ML workload" /><link rel="canonical" href="https://hetong07.github.io/posts/GPU-Scheduling/" /><meta property="og:url" content="https://hetong07.github.io/posts/GPU-Scheduling/" /><meta property="og:site_name" content="hetong07" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-01-16T06:00:00+08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="GPU Scheduling for ML workload" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@hetong07" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"description":"GPU Scheduling for ML workload","url":"https://hetong07.github.io/posts/GPU-Scheduling/","@type":"BlogPosting","headline":"GPU Scheduling for ML workload","dateModified":"2021-01-19T09:35:44+08:00","datePublished":"2021-01-16T06:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://hetong07.github.io/posts/GPU-Scheduling/"},"author":{"@type":"Person","name":"hetong07"},"@context":"https://schema.org"}</script><title>GPU Scheduling for ML workload | hetong07</title><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="cdn.jsdelivr.net"><link rel="dns-prefetch" href="cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"><link rel="preload" href="/assets/css/post.css" as="style"><link rel="stylesheet" href="/assets/css/post.css"><link rel="preload" as="style" href="/assets/css/lib/bootstrap-toc.min.css"><link rel="stylesheet" href="/assets/css/lib/bootstrap-toc.min.css" /> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script async src="/assets/js/post.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/sample/avatar.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">hetong07</a></div><div class="site-subtitle font-italic">Personal reading posts</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/tabs/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tabs/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/tabs/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/tabs/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/hetong07" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" class="order-4" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['hetong07','gmail.com'].join('@')" aria-label="email" class="order-5" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" class="order-6" > <i class="fas fa-rss"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>GPU Scheduling for ML workload</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>GPU Scheduling for ML workload</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Sat, Jan 16, 2021, 6:00 AM +0800" > Jan 16 <i class="unloaded">2021-01-16T06:00:00+08:00</i> </span> by <span class="author"> hetong07 </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Mon, Jan 18, 2021, 5:35 PM -0800" > Jan 18 <i class="unloaded">2021-01-19T09:35:44+08:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2574 words">14 min</span></div></div><div class="post-content"><h1 id="gpu-scheduling-for-ml-workload">GPU Scheduling for ML workload</h1><p>GPU scheduling topic is very popular in both academia and industry. Previous focus is on distributed ML system, and key of those researches are the fact in ML training, the parameters do not require strong consistency. The solution is the Parameter server. Therefore, the key for scheduling multiple ML tasks within one cluster is also the ML properties.</p><h2 id="workload-properties">Workload properties</h2><p>The ML workflow can be divided into two categories: 1) serving tasks and 2) training tasks. For serving tasks, there usually has a SLO, while for training, the bandwidth is more critical (batch processing), and can be terminated abruptly. Due to the different requirements, people usually dedicate cluster for training and serving, respectively.</p><h2 id="gpu-properties">GPU properties</h2><p>Compared to CPU, the memory size of GPU is very limited, tens of GB compared to hundreds of GB. This may incur problem of placing models in GPU: either the model cannot fit, or frequent memory exchange hurts the performance. Moreover, creating a GPU context is also time-consuming.</p><h2 id="pipeswitch-scheduling-in-one-machine"><a href="https://www.usenix.org/conference/osdi20/presentation/bai">Pipeswitch</a>: scheduling in one machine</h2><p>It focuses on how to scheduling ML workload inside one machine. As mentioned above, the memory limitation, the increasing model size and the strict SLO pose tough requirement on scheduling. The solution consists of 1) Exploit the layer computation feature to divide module into different subgroup to create a computation pipeline to hide memory transfer; 2) Use double buffer (actually, triple buffers) to amortize the GPU initialization; 3) Customized memory management: 1) memory usage is invariant. 2) training tasks use memory in a FILO order (due to BP); 4) Pin GPU memory and allocate in customized manager; 5) Also store models in above manager to reduce module storage in memory. 6) Use CPU IPC to notify GPU that data is ready for next pipeline stage since GPU IPC is slow; 7) Reduce memory usage by separating model and its parameters.</p><p>My understanding of 6) is : the worker is a CPU side program which responsible for call GPU API to send data to GPU and start job, and the daemon is in charge of allocating memory. So to avoid using GPU IPC, the daemon only needs to notify worker instead of GPU.</p><p>Frankly speaking, the approach used in the paper is not novel, and the major drawback is very obvious: isolation. It also lack of considering for tasks that need multiple GPU. There would be low-utilization issue because the GPU is not spatially shared by different tasks.</p><h2 id="antman"><a href="https://www.usenix.org/conference/osdi20/presentation/xiao">Antman</a></h2><p>Different from Pipeswitch, Antman considering for scheduling GPU workload usually needs multiple GPUs. The motivations are</p><ul><li>Cluster utilization is low<li>Jobs wait time is proportional to the number of GPUs it requires. 4 GPUs job may wait longer than 2 GPU jobs; simply reserve GPUs cannot solve this and it may further decrease utilization;<li>Memory demands varies during the lifetime of a task ( I think <strong>Pipeswitch</strong> would not agree :) ) and may cause interference for tasks co-located in the same GPU;</ul><h3 id="memory-management">memory management</h3><p>The key property this paper leverage is the mini-batch: the memory utilization would drop significantly on the boundary of different mini-batches. At the starting point of each mini-batch, it adjust the GPU memory limit of each job based on its demands ( the number of tensors placed in CPU memory). As I can see, this approach could not totally solve the interference, and if one job continuously expands its memory usage, it would starve/hurts the co-located job. The authors seem to argue that mini-batch guarantees even if the starvation happens, it lasts very shortly due to the mini-batch properties.</p><h3 id="scheduling">scheduling</h3><p>To guard the SLO, the authors provides assign priority for different jobs and create a manager to scheduler to scheduling jobs. The scheduler also use the mini-batch info to predict which GPU is about to idle. To schedule a LE job it does:</p><ul><li>Allocate job in idle GPU, (optional) and guarantee GPUs are all “close”;<li>Reserve resource if No. of GPUs cannot be satisfied;</ul><p>For BE job, what the scheduler does are:</p><ul><li>If the job has been detained for a long time, find a low utilized GPU (not idle) to allocate job<li>Local coordinator limit BE job’s memory and SM to reduce interference and gradually increase them it does not hurts the LE job’s performance (mini-batch processing time)<li>Also consider the interference severity between different jobs.<li>If possible, promote BE to LE.</ul><h3 id="some-thoughts">Some thoughts</h3><ul><li>I guess the reason for BE job upgrade is the possibility of starvation of BE job if LE requests are too many;<li>I am curious that if it is possible to co-locate multiple (&gt;=2) LE jobs in one GPU because LE job may have more predicable memory usage;<li>It seems that all the control signals are gathered from application (framework) level, it may not be a problem since mini-batch processing time is usually hundreds of milliseconds;<li>Mini-batch and the co-location properties are manually tuned, so if there is an new model emerges, this method may not idle in a short time;<li>Still cannot solve the resource reservation caused low utilization issue. Maybe we can schedule some BE job to the reserved resources?<li>More system level resource limitation is needed in GPU world, e.g. cgroup?<li>Could we utilize the properties of training job (BW critical, more memory, stop abruptly) and serving job (latency critical, less memory, long running)?</ul><h2 id="heterogeneous-scheduling"><a href="https://www.usenix.org/conference/osdi20/presentation/narayanan-deepak">Heterogeneous scheduling</a></h2><p>This paper’s novelty is that, as far as I know, it is the first paper to address the interconnection issue (Network, PCIe, QPI) as a whole for ML workload. It also proposes the advantage of CPU in ML workload.</p><h3 id="background">Background</h3><p>There are two types of distributed ML systems:</p><ol><li>All-reduce<blockquote><p>In a task that uses all-reduce, only GPU machines are involved. In an iteration, GPUs compute the gradients of the model parameters independently, and then aggregate gradients using the all-reduce primitive.</p></blockquote><li>Parameter Server.<blockquote><p>In PS tasks, both GPU machines and CPU machines can be used. Different from all-reduce, the gradients are sent to PS, which typically runs on CPU machines and aggregates the received gradients. PS then runs certain DNN training optimizer, e.g., SGD or Adam and sends back the updated model</p></blockquote></ol><p>The core difference is All-reduce is a P2P structure (more specific, <strong>ring</strong> structure), while the PS is a master-slave architecture.</p><p>The authors argue that the CPU usage is relatively low in GPU clusters.</p><h3 id="solution">Solution</h3><p>The authors prove the theoretical optimal configuration under different situations, such as, with and without extra CPU available (omit here), and create a scheduler based on the analysis. More interesting part in this paper is to consider the intra-machine communication part. It take the PCI, NVlink etc. into account. The basic idea here is to constrain the data exchange within the <strong>bottleneck region</strong> to mitigate the bandwidth contention, especially:</p><ul><li><p>PCIe only The bottleneck here is the communication across the PCIe switch. So the remedy it. the authors let the GPU within the same PCIe switch to do reduce first, and then send data to CPU, and in turn, to NIC.</p><li><p>NVlink The bottleneck is the NIC to CPU. So it use the NVLink to exchange data, and let the final aggregator to send data to NIC to reduce contention.</p></ul><p>Some other analysis in this paper involves 1) why the divide the SS and CS and how to support async in this dividend; 2) Use pipeline to overlap processing time; 3) Amortize RDMA WRITE round trip; 4) RDMA related issues (<strong>interesting, you would not know in other places!!!</strong></p><h3 id="some-thoughts-1">Some thoughts</h3><p>I think the novelty in this paper is that it provides the theoretical analysis of the communication pattern in GPU scheduling, and it is the first, as far as I know, to consider the network and connection as whole. It also provides some very interesting knowledge related to RDMA. Such knowledge can be found in a NSDI paper from Microsoft two years ago.Beyond that, it also contributes to the PS architecture by splitting the SS and CS and place them on CPU and GPU, respectively. I am glad to learn so much internal knowledge of ML algorithms.</p><h2 id="hived-gpu-affinity-scheduling"><a href="https://www.usenix.org/conference/osdi20/presentation/zhao-hanyu">HiveD: GPU affinity scheduling</a></h2><h3 id="background-1">Background</h3><p>The authors found that current scheduling design only consider the number of GPU required by a job, however, due to many reasons, mostly the network and interconnection I think, if the GPU affinity is not considered, the performance will degrade. This observation is a little bit similar to the Bytedance paper (in terms of interconnection), but the authors focus on another aspect.</p><p>In current GPU cluster, users usually have the ability toe assign the affinity to a job, but due to the fragment, the affinity may not be fulfill, so a job can be starved. The authors further argue that reducing the fragment through scheduler is not easy to achieve since existing scheduler is already complicated. So they build a separate scheduler and use multi-level scheduling to solve this problem.</p><h3 id="solution-1">Solution</h3><blockquote><p>HiveD proposes to guarantee sharing safety (i.e., eliminating sharing anomaly as described in §2) as a prerequisite of sharing a GPU cluster. Specifically, if a sequence of GPU requests with affinity requirements can be satisfied in a private cluster, it should be satisfied in the corresponding virtual private cluster and the shared physical cluster.</p></blockquote><p><strong>The goal of this paper is to guarantee that job runs on cluster will not perform worse (in terms of queuing delay) compared to on the private cluster.</strong> The solution is very simple: it divides the GPU into 4 different layers: GPU (level-1), PCIe switch (level-2), CPU socket (level-3), and node levels (level-4), and exposes the structure of the GPU cluster to the schedulers. The scheduler is then only need to consult the structure assigned to users and try to allocate GPUs on different layers. Spiting and merging GPUs at different layer is exploit to reduce resource fragment.</p><p>It also has a preempt based priority support to improve GPU utilization, but to reduce the preempt chance, it prefers to allocate low priority job to cluster and is farthest away high priority jobs and place high priority jobs with less low priority jobs as possible.</p><p>For failure recover, it employed the decentralized way to store the status and reconstruct the status during recover ( related to k8s knowledge).</p><h3 id="some-thoughts-2">Some thoughts</h3><ul><li>Exposing structure of GPU to upper layer is a usually a good way to solve complicated problem, but it requires the users or administrators to manually specify the GPU structure, which I think, would pose burden to them;<li>Exposing the structure fails to consider the interconnection interference within the same machine (as discussed in the <em>Heterogeneous scheduling</em>);<li>The allocation strategy (place low and high jobs as separately as possible) seems to cause low utilization, so multi-tenet on one GPU may be needed;<li>The benefit of this paper is that it reduce/restrict the GPU fragment, no more;<li>The performance of this solution highly depends on the jobs and the VC assignment.</ul><h2 id="heterogeneity-aware-scheduling"><a href="https://www.usenix.org/conference/osdi20/presentation/narayanan-deepak">Heterogeneity-Aware Scheduling</a></h2><h3 id="problems">Problems</h3><p>This paper identifies three problems in state-of-the-art GPU scheduling framework:</p><ul><li><strong>Performance Heterogeneity</strong> There are various type of accelerators, even GPU has different generations;<li><strong>Generality across Policies</strong> Different users has different optimization goal, but existing framework embedded with specific policy, making them hard to extend;<li><strong>Co-location and Placement Optimizations</strong> Co-location can improve utilization, but interference should be considered.</ul><h3 id="solution-2">Solution</h3><p>This paper argues that we should explicitly schedule the tasks on various resources, e.g. GPU, TPU, FPGA. It needs to model the job allocation on different accelerators as an allocation matrix, X (the variant to solve), and the performance (in term of throughput) on different accelerators as T. It is noted that, T also contains the throughput if different jobs are spatial sharing an accelerator. The throughput matrix can be get from a pre-profiled database or profiled on the fly, and it also proposes a fingerprint profile matching solution using matrix completion.</p><p>After solving the optimization problem in above model, the scheduler is in charge to move the allocation as close to the optimal solution as possible. This is achieved through a priority score matrix computed by the time spent on specific job on specific machine (see &amp;5).</p><p>The scheduling happens in stages (rounds), where the stages are divided by mini-batch (similar to that of Gandiva). to remedy the starvation problem. Considering scheduling with space sharing (SS) enabled is a NP-hard problem, the authors develop a greedy solution: 1) find the highest priority job and check whether its requirement can be fulfilled. if no, find the the next priority job, else 2) schedule this job and remove all jobs assignment contains this job to ensure the same job will not be scheduled more than once in one round.</p><h3 id="some-thoughts-3">Some thoughts</h3><ul><li>Section is of high value, need read more carefully;<li>What is the matrix completion discussed in section 6?<li>In experiments, it seems this solution is slightly better than Gandiva, I think it is reasonable, since its solution is similar to Gandiva, but the allocation is proved to be better than that of Gandiva;<li>Due to the current ML workload properties, this paper only consider space sharing of two jobs. This is OK now, but may be in the future it would encounter a scalability problem if multiple sharing is permitted, and as far as I know, solving optimal problem is time-consuming;<li>Compared to the <strong>HiveD</strong>, it does not pose burden to cluster assignment, so it would be a better solution;<li>Need revisit this paper in the future.</ul><h2 id="gandiva"><a href="https://www.usenix.org/conference/osdi18/presentation/xiao">Gandiva</a></h2><h3 id="background-2">Background</h3><p>Gandiva is one the early solution to solve the ML training workload scheduling problem. And it also identifies some ML training workload properties as</p><ul><li>Training workload often needs early feedback;<li>GPU affinity is important;<li>Memory usage reduced greatly on the boundary of two mini-batches;</ul><h3 id="solution-3">Solution</h3><p>This paper uses multiple to solve this problem:</p><ul><li>Time-slicing: multiple job that fit in one node shares the GPU resources through suspend-resuming techniques;<li>Packing: Spatial sharing GPU resources; To remedy interference problem, it measures the mini-batch processing time and memory usage to know whether two jobs can be co-located, i.e., if after scheduling, the processing increase, then this assignment should be revoked.<li>Grow-Shrink: Speculatively execute jobs on low utilization machines.<li>Migration: migration to remedy the fragment issue. migration can be done in &lt; 4 seconds.</ul><p>When assign a job, the scheduler first tries to schedule it on minimal load nodes (prefer nodes with no load GPUs). If it cannot fulfill, the algorithm then gradually relax the affinity requirement. If there is no GPUs available, it migrates existing jobs to create usable resources. So migration is the least thing to do in scheduling. This approach make the job to run as fast as possible to fulfill the feature of training jobs.</p><h3 id="some-thoughts-4">Some thoughts</h3><ul><li>The reason in section 6.4, the Ganvida can outperforms the YARN is due to the fact the the GPU requirement cannot be fulfill. It seems the advance of Ganvida cannot be totally attributed to migration.<li>Migration even on the basis of mini-batch is still time consuming, and considering the priority issue, it may causes it infeasible to fulfill next high priority jobs. Moreover, frequently migration may cause oscillation, which may hurts the performance even further.</ul><h2 id="others">Others</h2><ul><li>I remember that Prof. Arvind team has a <a href="https://homes.cs.washington.edu/~arvind/papers/nexus.pdf">paper</a> talks about providing SLO for serving ML workload. It is also interesting since it points out that bin bucket is not sufficient for serving ML workflow with SLO on GPU cluster. I have forgotten its main idea, but I will read it again and add my understanding here later.<li>I haven’t read <a href="https://www.usenix.org/conference/nsdi19/presentation/gu">Tiresias</a>.</ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/blogging/'>Blogging</a>, <a href='/categories/technical/'>Technical</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/gpu/" class="post-tag no-text-decoration" >GPU</a> <a href="/tags/ml-sys/" class="post-tag no-text-decoration" >ML Sys</a> <a href="/tags/distributed-system/" class="post-tag no-text-decoration" >distributed system</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=GPU Scheduling for ML workload - hetong07&url=https://hetong07.github.io/posts/GPU-Scheduling/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=GPU Scheduling for ML workload - hetong07&u=https://hetong07.github.io/posts/GPU-Scheduling/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=GPU Scheduling for ML workload - hetong07&url=https://hetong07.github.io/posts/GPU-Scheduling/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/GPU-Scheduling/">GPU Scheduling for ML workload</a><li><a href="/posts/caladan/">Caladan interference mitigation</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/distributed-system/">distributed system</a> <a class="post-tag" href="/tags/paxos/">paxos</a> <a class="post-tag" href="/tags/raft/">raft</a> <a class="post-tag" href="/tags/gpu/">GPU</a> <a class="post-tag" href="/tags/interference-mitigation/">interference mitigation</a> <a class="post-tag" href="/tags/kv-store/">KV store</a> <a class="post-tag" href="/tags/leveldb/">leveldb</a> <a class="post-tag" href="/tags/ml-sys/">ML Sys</a> <a class="post-tag" href="/tags/rsm/">RSM</a> <a class="post-tag" href="/tags/scheduling/">scheduling</a></div></div></div><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/paxos-and-raft/"><div class="card-body"> <span class="timeago small" > May 26, 2020 <i class="unloaded">2020-05-26T10:42:00+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Paxos and Raft</h3><div class="text-muted small"><p> This is not a complete explanation of the Paxos and the Raft, but just some thoughts about some corner cases. Livelock and leader in the Paxos The basic paxos algorith works well but has a liveloc...</p></div></div></a></div><div class="card"> <a href="/posts/Paxos-RSM/"><div class="card-body"> <span class="timeago small" > May 28, 2020 <i class="unloaded">2020-05-28T06:09:00+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Paxos RSM</h3><div class="text-muted small"><p> Replicated State Machine (RSM) [1] talks about the theory of RSM. The core idea if the RSM is straightforward: replicate the state machine (SM) on different processes/servers, and as lone as the in...</p></div></div></a></div><div class="card"> <a href="/posts/Basic-Understanding-of-LevelDB/"><div class="card-body"> <span class="timeago small" > Jun 7, 2020 <i class="unloaded">2020-06-07T14:22:00+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Basic Understanding of LevelDB</h3><div class="text-muted small"><p> LevelDB is the open-sourced Google BigTable implementaion, and its coding style is nice for amateurs to follow. This post presents my understands of the LevelDB and some details of its implementaio...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/caladan/" class="btn btn-outline-primary"><p>Caladan interference mitigation</p></a> <span class="btn btn-outline-primary disabled"><p>-</p></span></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2021 <a href="https://twitter.com/username">hetong07</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy/" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-xl-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/distributed-system/">distributed system</a> <a class="post-tag" href="/tags/paxos/">paxos</a> <a class="post-tag" href="/tags/raft/">raft</a> <a class="post-tag" href="/tags/gpu/">GPU</a> <a class="post-tag" href="/tags/interference-mitigation/">interference mitigation</a> <a class="post-tag" href="/tags/kv-store/">KV store</a> <a class="post-tag" href="/tags/leveldb/">leveldb</a> <a class="post-tag" href="/tags/ml-sys/">ML Sys</a> <a class="post-tag" href="/tags/rsm/">RSM</a> <a class="post-tag" href="/tags/scheduling/">scheduling</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://hetong07.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"><div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>{categories}</div><div><i class="fa fa-tag fa-fw"></i>{tags}</div></div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>' }); </script>
